<?xml version="1.0"?><data><abstract><p>Variability in quantitative gait data arises from many potential sources, including natural temporal dynamics of neuromotor control, pathologies of the neurological or musculoskeletal systems, the effects of aging, as well as variations in the external environment, assistive devices, instrumentation or data collection methodologies. In light of this variability, unidimensional, cycle-based gait variables such as stride period should be viewed as random variables and prototypical single-cycle kinematic or kinetic curves ought to be considered as random functions of time. Within this framework, we exemplify some practical solutions to a number of commonly encountered analytical challenges in dealing with gait variability. On the topic of univariate gait variables, robust estimation is proposed as a means of coping with contaminated gait data, and the summary of non-normally distributed gait data is demonstrated by way of empirical examples. On the summary of gait curves, we discuss methods to manage undesirable phase variation and non-robust spread estimates. To overcome the limitations of conventional comparisons among curve landmarks or parameters, we propose as a viable alternative, the combination of curve registration, robust estimation, and formal statistical testing of curves as coherent units. On the basis of these discussions, we provide heuristic guidelines for the summary of gait variables and the comparison of gait curves.</p></abstract><body xmlns:ns0="http://www.w3.org/1999/xlink"><sec><title>Introduction</title><sec><title>Definition of variability</title><p>In quantitative gait analysis, variability is commonly understood to be the fluctuation in the value of a kinematic (e.g. joint angle), kinetic (e.g. ground reaction force), spatio-temporal (e.g. stride interval) or electromyographic measurement. This fluctuation may be observed in repeated measurements over time, across or within individuals or raters, or between different measurement, intervention or health conditions. In this paper, we will focus on the variability in two types of data: unidimensional gait variables and single-cycle, prototypical gait curves, as these are the most common abstractions of spatio-temporal, kinematic and kinetic data, typically collected within a gait laboratory.</p></sec><sec><title>Measurement</title><p>Many different analytical methods have been proposed for estimating the variability in gait variables. The most widely used measures are those relating to the second moment of the underlying probability distribution of the gait variable of interest. Examples include, standard deviation (e.g., [<xref ref-type="bibr" rid="B1">1</xref>-<xref ref-type="bibr" rid="B4">4</xref>]), coefficient of variation (e.g., [<xref ref-type="bibr" rid="B5">5</xref>-<xref ref-type="bibr" rid="B8">8</xref>]) and coefficient of multiple correlation (e.g., [<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B10">10</xref>]). Other less conventional variability measures have also been suggested. For example, Kurz et al. demonstrated an information-theoretic measure of variability, where increased uncertainty in joint range-of-motion (ROM), and hence entropy, reflected augmented variability in joint ROM [<xref ref-type="bibr" rid="B11">11</xref>].</p><p>For gauging variability among gait curves, some distance-based measures have been put forth, including the mean distance from all curves to the mean curve in raw 3-dimensional spatial data [<xref ref-type="bibr" rid="B12">12</xref>], the point-by-point intercurve ranges averaged across the gait cycle [<xref ref-type="bibr" rid="B13">13</xref>] and the norm of the difference between coordinate vectors representing upper and lower standard deviation curves in a vector space spanned by a polynomial basis [<xref ref-type="bibr" rid="B14">14</xref>]. Instead of reporting a single number, an alternative and popular approach to ascertain curve variability has been to peg prediction bands around a group of curves. Recent research on this topic has demonstrated that bootstrap-derived prediction bands provide higher coverage than conventional standard deviation bands [<xref ref-type="bibr" rid="B15">15</xref>-<xref ref-type="bibr" rid="B17">17</xref>].</p><p>Additionally, various summary statistics, such as the intra-class correlation coefficient [<xref ref-type="bibr" rid="B8">8</xref>] and Pearson correlation coefficient [<xref ref-type="bibr" rid="B18">18</xref>], for estimating gait measurement reliability, repeatability or reproducibility have been deployed in the assessment of methodological, environmental and instrumentation or device-induced variability. Principal components and multiple correspondence analyses have also been applied in the quantification of variability in both gait variables and curves, as retained variance and inertia, respectively, in low dimensional projections of the original data [<xref ref-type="bibr" rid="B19">19</xref>].</p></sec><sec><title>Sources of variability</title><p>As depicted in Figure <xref ref-type="fig" rid="F1">1</xref>, the numerous sources of variability in gait measurements can be loosely categorized as either internal or external to the individual being observed [<xref ref-type="bibr" rid="B20">20</xref>].</p><fig id="F1" position="float"><label>Figure 1</label><caption><p>Sources of variability in empirical gait measurements.</p></caption><graphic ns0:href="1743-0003-2-22-1" /></fig><sec><title>Internal</title><p>Internal variability is inherent to a person's neurological, metabolic and musculoskeletal health, and can be further subdivided into natural fluctuations, aging effects and pathological deviations. It is now well known that neurologically healthy gait exhibits natural temporal fluctuations that are governed by strong fractal dynamics [<xref ref-type="bibr" rid="B21">21</xref>-<xref ref-type="bibr" rid="B23">23</xref>]. The source of these temporal fluctuations may be supraspinal [<xref ref-type="bibr" rid="B24">24</xref>] and potentially the result of correlated central pattern generators [<xref ref-type="bibr" rid="B25">25</xref>]. One hierarchical synthesis hypothesis purports that these nonlinear dynamics are due to the neurological integration of visual and auditory stimuli, mechanoreception in the soles of the feet, along with vestibular, proprioceptive and kinesthetic (e.g., muscle spindle, Golgi tendon organ and joint afferent) inputs arriving at the brain on different time scales [<xref ref-type="bibr" rid="B24">24</xref>,<xref ref-type="bibr" rid="B26">26</xref>]. Internal variability in gait measurements may be altered in the presence of pathological conditions which affect natural bipedal ambulation. For example, muscle spasticity tends to augment within-subject variability of kinematic and time-distance parameters [<xref ref-type="bibr" rid="B10">10</xref>] while Parkinson's disease, particularly with freezing gait, leads to inflated stride-to-stride variability [<xref ref-type="bibr" rid="B27">27</xref>] and electromyographic (EMG) shape variability and reduced timing variability in the EMG of the gastrocnemius muscle [<xref ref-type="bibr" rid="B28">28</xref>]. Similarly, recent studies have reported increased stride-to-stride variability due to Huntington's disease [<xref ref-type="bibr" rid="B29">29</xref>], amplified swing time variability due to major depressive and bipolar disorders [<xref ref-type="bibr" rid="B30">30</xref>], and heightened step width [<xref ref-type="bibr" rid="B31">31</xref>] and stride period [<xref ref-type="bibr" rid="B32">32</xref>] variability due to natural aging of the locomotor system.</p></sec><sec><title>External</title><p>Aside from mechanisms internal to the individual, variability in gait measurements may also arise from various external factors, as shown in Figure <xref ref-type="fig" rid="F1">1</xref>. For example, influences of the physical environment, such as the type of walking surface [<xref ref-type="bibr" rid="B33">33</xref>], the level of ambient lighting in conjunction with type of surface [<xref ref-type="bibr" rid="B34">34</xref>] and the presence and inclination of stairs [<xref ref-type="bibr" rid="B35">35</xref>] have been shown to affect cadence, step-width, and ground reaction force variability, respectively, in certain groups of individuals. Assistive devices, such as canes or semirigid ankle orthoses may reduce step-time and step-width variability [<xref ref-type="bibr" rid="B36">36</xref>] while different footwear (soft or hard) can affect the variability of knee and ankle joint angles, possibly by altering peripheral sensory inputs [<xref ref-type="bibr" rid="B14">14</xref>].</p><p>Variability may also originate from the nature of the instrumentation employed. This variability is often appraised by way of test-retest reliability studies. Some recent examples include the reproducibility of measurements made with the GAITRite mat [<xref ref-type="bibr" rid="B8">8</xref>], 3-dimensional optical motion capture systems [<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B18">18</xref>], triaxial accelerometers [<xref ref-type="bibr" rid="B37">37</xref>], insole pressure measurement systems [<xref ref-type="bibr" rid="B4">4</xref>], and a global positioning system for step length and frequency recordings [<xref ref-type="bibr" rid="B7">7</xref>].</p><p>Experimenter error or inconsistencies may also contribute, as an external source, to the observed variability in gait data. Besier et al. contend that the repeatability of kinematic and kinetic models depends on accurate location of anatomical landmarks [<xref ref-type="bibr" rid="B38">38</xref>]. Indeed, various studies have confirmed the exaggerated variability in kinematic data due to differences in marker placement between trials [<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B39">39</xref>] and between raters [<xref ref-type="bibr" rid="B40">40</xref>]. Finally, analytical manipulations, such as the computation of Euler angles [<xref ref-type="bibr" rid="B9">9</xref>] or the estimation of cross-sectional averages [<xref ref-type="bibr" rid="B41">41</xref>] may also amplify the apparent variability in gait data.</p></sec></sec><sec><title>Clinical significance of variability</title><p>The magnitude of variability and its alteration bears significant clinical value, having been linked to the health of many biological systems. Particularly in human locomotion, the loss of natural fractal variability in stride dynamics has been demonstrated in advanced aging [<xref ref-type="bibr" rid="B32">32</xref>] and in the presence of neurological pathologies such as Parkinson's disease [<xref ref-type="bibr" rid="B42">42</xref>], and amyotrophic lateral sclerosis [<xref ref-type="bibr" rid="B42">42</xref>]. In some cases, this fractal variability is correlated to disease severity [<xref ref-type="bibr" rid="B32">32</xref>]. Variability may also serve as a useful indicator of the risk of falls [<xref ref-type="bibr" rid="B43">43</xref>] and the ability to adapt to changing conditions while walking [<xref ref-type="bibr" rid="B44">44</xref>]. Stride-to-stride temporal variability may be useful in studying the developmental stride dynamics in children [<xref ref-type="bibr" rid="B45">45</xref>]. Natural variability has been implicated as a protective mechanism against repetitive impact forces during running [<xref ref-type="bibr" rid="B14">14</xref>] and possibly a key ingredient for energy efficient and stable gait [<xref ref-type="bibr" rid="B46">46</xref>]. Variability is not always informative and useful and in fact may lead to discrepancies in treatment recommendations. For example, due to variability in static range-of-motion and kinematic measurements, Noonan et al. found that different treatments were recommended for 9 out of 11 patients with cerebral palsy, examined at four different medical centres [<xref ref-type="bibr" rid="B13">13</xref>].</p></sec><sec><title>Dealing with variability</title><p>Given the ubiquity and health relevance of variability in gait measurements, it is critical that we summarize and compare gait data in a way that reflects the true nature of their variability. Despite the apparent simplicity of these tasks, if not conducted prudently, the derived results may be misleading, as we will exemplify. In fact, there are to date many open questions relating to the analysis of quantitative gait data, such as the elusive problem of systematically comparing two families of curves.</p><p>The objectives of this paper are twofold. First, we aim to review some of the analytical issues commonly encountered in the summary and comparison of gait data variables and curves, as a result of variability. Our second goal is to demonstrate some practical solutions to the selected challenges, using real empirical data. These solutions largely draw upon successful methods reported in the statistics literature. The remainder of the paper addresses these objectives under two major headings, one on gait variables and the other on gait curves. The paper closes with some suggestions for the summary and comparison of gait data and directions for future research on this topic.</p></sec></sec><sec><title>Gait random variables</title><p>Unidimensional variables which are measured or computed once per gait cycle will be referred to as gait random variables. This category includes spatio-temporal parameters such as stride length, period and frequency, velocity, single and double support times, and step width and length, as well as parameters such as range-of-motion of a particular joint, peak values, and time of occurrence of a peak, which are extracted from kinematic or kinetic curves on a per cycle basis.</p><p>Due to variability, univariate gait measures and parameters derived thereof should be regarded as stochastic rather than deterministic variables [<xref ref-type="bibr" rid="B47">47</xref>,<xref ref-type="bibr" rid="B48">48</xref>]. In this random variable framework, a one-dimensional gait variable is represented as <bold>X </bold>and governed by an underlying, unknown probability distribution function <italic>F</italic><sub><italic>X</italic></sub>, or density function <inline-graphic ns0:href="1743-0003-2-22-i1.gif" />. A realization of this random variable is written in lower case as <italic>x</italic>.</p><sec><title>Inflated variability and non-robust estimation</title><p>It has been recently demonstrated that typical location and spread estimators used in quantitative gait data analysis, i.e. mean and variance, are highly susceptible to small quantities of contaminant data [<xref ref-type="bibr" rid="B48">48</xref>]. Indeed, a few spurious or atypical measurements can unduly inflate non-robust estimates of gait variability. The challenge in the summary of highly variable univariate gait data lies in reporting location and spread, faithful to the underlying data distribution and minimally influenced by extraordinary observations.</p><p>Here, we focus on the issue of inflated variability and non-robust estimation by examining four different spread estimators, applied to stride period data from a child with spastic diplegic cerebral palsy. As stated above, the coefficient of variation and standard deviation are routinely employed in the summary of gait variables. Given a sample of <italic>N </italic>observations of a gait variable <bold>X</bold>, i.e., {<italic>x</italic><sub>1</sub>,..., <italic>x</italic><sub><italic>N</italic></sub>}, the coefficient of variation is defined as,</p><p><inline-graphic ns0:href="1743-0003-2-22-i2.gif" /></p><p>where the numerator is simply the sample standard deviation and the denominator, <inline-graphic ns0:href="1743-0003-2-22-i3.gif" />, is the sample mean. We also include two other estimators, although seldom used in gait analysis, to illustrate the qualitative differences in estimator robustness. The interquartile range of the sample is defined as</p><p><italic>IQR</italic>(<bold>X</bold>) = <italic>x</italic><sub>0.75 </sub>- <italic>x</italic><sub>0.25 </sub>&#160;&#160;&#160; (2)</p><p>where <italic>x</italic><sub>0.75 </sub>and <italic>x</italic><sub>0.25 </sub>are the 75% and 25% quantiles. The <italic>q</italic>-quantile is defined as <inline-graphic ns0:href="1743-0003-2-22-i4.gif" /> where as usual, <italic>F</italic><sub><italic>X </italic></sub>is the probability distribution of <bold>X</bold>. Equivalently, the <italic>q</italic>-quantile is the value, <italic>x</italic><sub><italic>q</italic></sub>, of the random variable where <inline-graphic ns0:href="1743-0003-2-22-i5.gif" />. That is, <italic>q </italic>&#215; 100 percent of the random variable values lie below <italic>x</italic><sub><italic>q</italic></sub>. We also introduce the median absolute deviation [<xref ref-type="bibr" rid="B49">49</xref>],</p><p><italic>MAD</italic>(<bold>X</bold>) = med (|<bold>X </bold>- med(<bold>X</bold>)|) &#160;&#160;&#160; (3)</p><p>where med(<italic>X</italic>) is the median of the sample, or the 50% quantile as defined above. This last estimator is, as the name implies, the median of the absolute difference between the sample values and their median value. We are interested in studying how these different estimators perform when estimating the spread in a gait variable, the observations of which may contain outlying values or contaminants. In the left pane of Figure <xref ref-type="fig" rid="F2">2</xref>, we show a set of stride period data recorded from a child with spastic diplegia. The top graph shows the raw data with a number of obvious outliers with atypically long stride times. We adopted a common outlier definition, labeling points more than 1.5 interquartile ranges away from the sample median as extreme values. According to this definition there were 21 outlying observations. In the bottom graph, the outliers have been removed. The bar graph on the right-hand side of Figure <xref ref-type="fig" rid="F2">2</xref> portrays the spread estimates of the stride period data, computed with each estimator introduced above, with and without the outliers.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p>Robust vs. non-robust estimators of parameter spread. The left pane shows a sequence of stride periods with outliers (top) and after removal of outliers (bottom). The right pane is a bar graph showing the values of four different spread estimators before and after outlier removal.</p></caption><graphic ns0:href="1743-0003-2-22-2" /></fig><p>We note immediately that the spread estimates in the presence of outliers are higher. The standard deviation and coefficient of variation change the most, dropping 42 and 36 percent in value, respectively, upon outlier removal. This observation is particularly important in the comparison of gait variables, as inflated variability estimates will diminish the probability of detecting significant differences when they do in fact exist. In contrast, the interquartile range and median absolute deviation, only change by 21 and 11%, respectively. We see that these latter estimates are more statistically stable, in that they are not as greatly influenced by the presence of extreme observations.</p><p>To more fully comprehend estimator robustness or lack thereof, the field of robust statistics offers a valuable tool called influence functions, which as the name implies, summarizes the influence of local contaminations on estimated values. Their use in gait analysis was first introduced in the context of stride frequency estimation [<xref ref-type="bibr" rid="B48">48</xref>].</p><p>We first introduce the concept of a functional, which can be understood as a real-valued function on a vector space of probability distributions [<xref ref-type="bibr" rid="B50">50</xref>]. In the present context, functionals allow us to think of an estimator as a function of a probability distribution. For example, for the interquartile range, the functional is simply, <inline-graphic ns0:href="1743-0003-2-22-i6.gif" />.</p><p>Let the mixture distribution <italic>F</italic><sub><italic>z</italic>, <italic>&#949; </italic></sub>describe data governed by distribution <italic>F </italic>but contaminated by a sample <italic>z</italic>, with probability <italic>&#949;</italic>. The influence function at the contamination <italic>z </italic>is defined as</p><p><inline-graphic ns0:href="1743-0003-2-22-i7.gif" /></p><p>where <italic>T</italic>(&#183;) is the functional for the estimator of interest. The influence function for a particular estimator measures the incremental change in the estimator, in the presence of large samples, due to a contamination at <italic>z</italic>. Clearly, if the impact of this contaminant on the estimated value is minimal, then the estimator is locally robust at <italic>z</italic>. Influence functions can be analytically derived for a variety of common gait estimators (see for example, [<xref ref-type="bibr" rid="B48">48</xref>]), including those mentioned above. For the sake of analytical simplicity and practical convenience, we will instead use finite sample sensitivity curves, <italic>SC</italic>(<italic>z</italic>), which can be defined as,</p><p><italic>SC</italic>(<italic>z</italic>) = (<italic>N </italic>+ 1){<italic>T</italic>(<italic>x</italic><sub>1</sub>,..., <italic>x</italic><sub><italic>N</italic></sub>, <italic>z</italic>) - <italic>T</italic>(<italic>x</italic><sub>1</sub>,..., <italic>x</italic><sub><italic>N</italic></sub>)} &#160;&#160;&#160; (5)</p><p>where as above, <italic>T</italic>(&#183;) is the functional for the estimator in question, and <italic>z </italic>is the contaminant observation. When <italic>N </italic>&#8594; &#8734; the sensitivity curve converges to the influence function for many estimators. Like the asymptotic influence functions, sensitivity curves describe the local impact of a contamination <italic>z </italic>on the estimator value. For the purposes of computer simulation, the functional <italic>T</italic>(<italic>x</italic><sub>1</sub>,..., <italic>x</italic><sub><italic>N</italic></sub>, <italic>z</italic>) and <italic>T</italic>(<italic>x</italic><sub>1</sub>,..., <italic>x</italic><sub><italic>N</italic></sub>) are simply the evaluations of the estimator of interest at the augmented and original samples, respectively. Figure <xref ref-type="fig" rid="F3">3</xref> depicts the sensitivity curves for the estimators introduced in the stride period example. To generate these curves, we used the cleansed stride period data (without outliers) and incrementally added a deviant stride period from 0.5 below the lowest sample value to 0.5 above the highest sample value. The sample mean for this data was 1.41 seconds.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p>Sensitivity curves for various estimators of gait parameter variability based on the stride period example.</p></caption><graphic ns0:href="1743-0003-2-22-3" /></fig><p>We observe that both standard deviation and coefficient of variation have quadratic sensitivity curves with vertices close to the sample mean. In other words, as contaminants take on extreme low or high values, the estimated values are unbounded. Clearly, these two estimators are not robust, explaining their high sensitivity to the outliers in the stride period data. In contrast, both the interquartile range and median absolute deviation have bounded sensitivity curves, in the form of step functions. The median absolute deviation is actually not sensitive to contaminant values above 1.1 seconds whereas the interquartile range has a constant sensitivity to contaminant values over 1.6. Since most of the outliers in the stride period data were well above the mean, this difference explains the considerably lower sensitivity of the median absolute deviation to outlier influence.</p><p>From this example, we appreciate that estimators of gait variable spread (i.e. variability) should be selected with prudence. The popular but non-robust variability measures of standard deviation and coefficient of variation both have 0 breakdown points [<xref ref-type="bibr" rid="B51">51</xref>], meaning that only a single extreme value is required to drive the estimators to infinity. Indeed, as seen in Figure <xref ref-type="fig" rid="F2">2</xref>, the presence of a small fraction of outliers can unduly inflate our estimates of gait variability. Outlier management [<xref ref-type="bibr" rid="B52">52</xref>], with methods such as outlier factors [<xref ref-type="bibr" rid="B53">53</xref>] or frequent itemsets [<xref ref-type="bibr" rid="B54">54</xref>], represents one possible strategy to reduce unwanted variability when using these non-robust estimators. Apart from the addition of a computational step, this strategy introduces the undesirable effects of outlier smearing and masking [<xref ref-type="bibr" rid="B55">55</xref>], which need to be carefully addressed.</p><p>In contrast, outliers need not be explicitly identified with robust estimation, hence circumventing the above complications and abbreviating computation. The interquartile range and median absolute deviation, have breakdown points of 0.25 and 0.5, respectively [<xref ref-type="bibr" rid="B51">51</xref>]. Practically, this means that these estimators will remain stable (bounded) until the proportion of outliers reaches 25% and 50% of the sample size, respectively. To circumvent explicit outlier detection and its associated issues altogether, and in the presence of noisy data, which often result from spatio-temporal recordings and parameterizations of kinematic and kinetic curves, robust estimators may thus be preferable in the summary of gait variables.</p></sec><sec><title>Non-gaussian distributions</title><p>Even in the absence of outliers, univariate gait data may not adhere to a simple, unimodal gaussian distribution. In fact, distributions of gait measurements and derived parameters may be naturally skewed, leptokurtic or multimodal [<xref ref-type="bibr" rid="B56">56</xref>]. Neglecting these possibilities, we may summarize gait data with location and spread values which do not reflect the underlying data distribution.</p><sec><title>Semi-parametric estimation</title><p>As an example, consider the hip range-of-motion extracted from 45 strides of 9 able-bodied children. A histogram of the data is plotted in Figure <xref ref-type="fig" rid="F4">4</xref>. Assuming that the data are gaussian distributed, we arrive at maximum likelihood estimates for the mean and standard deviation, i.e. 40.4 &#177; 5.1. However, the histogram clearly appears to be bimodal. A Lilliefors test [<xref ref-type="bibr" rid="B57">57</xref>] confirms significant departure from normality (<italic>p </italic>= 0.02). A number of approaches could be undertaken to find the underlying modes. One could perform simple clustering analysis [<xref ref-type="bibr" rid="B58">58</xref>], such as <italic>k</italic>-means clustering. Doing so reveals two well-defined clusters, the means and standard deviations of which are reported in Table <xref ref-type="table" rid="T1">1</xref>. Alternatively, one could attempt to fit to the data, a convex mixture density of the form,</p><fig id="F4" position="float"><label>Figure 4</label><caption><p>Multimodal parameter distribution. Shown here is a histogram of hip range-of-motion (45 strides from 9 able-bodied children) with two possible distribution functions overlaid: unimodal normal probability distribution (solid line) and bimodal gaussian mixture distribution (dashed line).</p></caption><graphic ns0:href="1743-0003-2-22-4" /></fig><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Summary of bimodal ROM data</p></caption><table frame="hsides" rules="groups"><thead><tr><td /><td align="center">Mixture distribution</td><td align="center"><italic>k</italic>-means clustering</td><td align="center">Normal distribution</td></tr></thead><tbody><tr><td align="center">Mode # 1</td><td align="center">37.7 &#177; 2.4</td><td align="center">37.7 &#177; 2.6</td><td align="center">40.4 &#177; 5.1</td></tr><tr><td align="center">Mode # 2</td><td align="center">49.1 &#177; 3.5</td><td align="center">47.7 &#177; 3.0</td><td align="center">-</td></tr><tr><td align="center">Mixing proportion (mode I/mode 2)</td><td align="center">0.71/0.29</td><td align="center">0.73/0.27</td><td align="center">-</td></tr><tr><td align="center">Critical value (lower)</td><td align="center">33.35</td><td align="center">32.96</td><td align="center">30.40</td></tr><tr><td align="center">Critical value (upper)</td><td align="center">53.89</td><td align="center">51.70</td><td align="center">50.40</td></tr></tbody></table></table-wrap><p><inline-graphic ns0:href="1743-0003-2-22-i8.gif" /></p><p>where <italic>W</italic><sub><italic>i </italic></sub>is a scalar such that &#8721;<sub><italic>i </italic></sub><italic>W</italic><sub><italic>i </italic></sub>= 1 to preserve probability axioms, <italic>N</italic><sub><italic>C </italic></sub>is the number of clusters or modes and <inline-graphic ns0:href="1743-0003-2-22-i9.gif" /> is a gaussian density with mean <italic>&#956;</italic><sub><italic>i </italic></sub>and variance <inline-graphic ns0:href="1743-0003-2-22-i10.gif" />. The fitting of (6) is known as semi-parametric estimation as we do not assume a particular parametric form for the data distribution per se, but do assume that it can modeled by a mixture of gaussians. In the present case, <italic>N</italic><sub><italic>C </italic></sub>= 2 and we can use a simple optimization approach to determine the parameters of the mixture. In particular, we determined the parameter vector [<italic>W</italic><sub>1</sub>, <italic>W</italic><sub>2</sub>, <italic>&#956;</italic><sub>1</sub>, <italic>&#963;</italic><sub>1</sub>, <italic>&#956;</italic><sub>2</sub>, <italic>&#963;</italic><sub>2</sub>] to minimize the objective function <inline-graphic ns0:href="1743-0003-2-22-i11.gif" />, where <italic>n</italic><sub><italic>j </italic></sub>is the number of points within an interval of length &#916; around <italic>x</italic><sub><italic>j </italic></sub>and <italic>N </italic>is the number of points in the sample. The latter term in the objective function is a crude probability density estimate [<xref ref-type="bibr" rid="B59">59</xref>]. As seen in Table <xref ref-type="table" rid="T1">1</xref>, the results of fitting this bimodal mixture yields similar results to those obtained from clustering.</p><p>What are the implications of naively summarizing these data with a unimodal normal distribution? First of all, the probabilities of observing range-of-motion values between 35 and 39 degrees, where most of the observations occur, would be underestimated. Likewise, ROM values between 39 and 48 degrees, where the data exhibit a dip in observed frequencies, would be grossly overestimated. These discrepancies are labeled as regions B and C in Figure <xref ref-type="fig" rid="F4">4</xref>. More importantly, the discrepancies in the tails of the distributions, regions A and D, suggest that statistical comparisons with other data, say pathological ROM, would likely yield inconsistent conclusions, depending on whether the mixture or simple distribution was assumed. Indeed, as seen in Table <xref ref-type="table" rid="T1">1</xref> the lower critical value of the simple normal distribution for a 5% significance level is too low. This could lead to exagerrated Type II errors. Similarly, the upper critical value is not high enough, potentially leading to many false positive (Type I) errors.</p><p>The above example depicts bimodal data. However, the mixture distribution method can be applied to arbitrary non-normal data distributions, regardless of the underlying modality. Fitting such distributions can be accomplished by the well-established expectation-maximization algorithm [<xref ref-type="bibr" rid="B60">60</xref>]. For a comprehensive review of other semi-parametric and non-parametric estimation methods, see for example [<xref ref-type="bibr" rid="B59">59</xref>].</p></sec><sec><title>Parametric estimation</title><p>When we have some <italic>a priori </italic>knowledge about the underlying data distribution, we can adopt a simpler approach to summarize the gait data. In particular, we could fit the data to a specific parametric form. As an example, consider the task of comparing two sets of stride period data from two children with spastic diplegia, with identical gross motor function classification scores [<xref ref-type="bibr" rid="B61">61</xref>]. The histograms of strides for both children are shown in Figure <xref ref-type="fig" rid="F5">5</xref>. It is known that stride period data tend to be right-skewed [<xref ref-type="bibr" rid="B56">56</xref>]. A careful examination of the bottom graph indicates that the histogram is indeed right-skewed. In fact, the skewness value is 1.7 and Lilliefors test for normality [<xref ref-type="bibr" rid="B57">57</xref>] confirms significant departure from normality (<italic>p </italic>&lt; 10<sup>-5</sup>). We thus determine the maximum likelihood gamma distribution for these data. The gamma distribution has the following parametric form [<xref ref-type="bibr" rid="B62">62</xref>],</p><fig id="F5" position="float"><label>Figure 5</label><caption><p>Comparison of stride period distributions between 2 children with spastic diplegia. In each graph, the dashed line is the normal probability distribution estimated for the data. The solid line is the gamma distribution fit to the data.</p></caption><graphic ns0:href="1743-0003-2-22-5" /></fig><p><inline-graphic ns0:href="1743-0003-2-22-i12.gif" /></p><p>where <italic>a </italic>is the shape parameter, <italic>b </italic>is the scale parameter and &#915;(&#183;) is the gamma function. The gamma distribution fits are plotted as solid lines in Figure <xref ref-type="fig" rid="F5">5</xref>.</p><p>As in the previous example, we consider the consequence of assuming that the data are normally distributed. Do these two children have similar stride periods? To answer this question, one may hastily apply a t-test, assuming that the stride period distributions are gaussian. The results of this test reveal no significant differences (<italic>p </italic>= 0.31), as reported in Table <xref ref-type="table" rid="T2">2</xref>. To visualize the departure from normality, the maximum likelihood normal probability distribution fits to the stride data are superimposed on each histogram as a dashed curve. Note that the tails of the distribution are overly broad, particularly in the bottom graph. This diminishes the likelihood of detecting genuine significant differences between the data sets. Table <xref ref-type="table" rid="T2">2</xref> summarizes the maximum likelihood estimates of the distribution parameters under the two different distributional assumptions. Under the gamma distribution assumption, the stride periods between the two children are statistically different (<italic>p </italic>= 0.036) according to a Monte Carlo simulation of differences between 10<sup>4 </sup>similarly distributed gamma random variables, which contradicts the previous conclusion. We have arbitrarily chosen the gamma distribution in this example as it appears to describe well the positively skewed data. However, there are many other parametric forms that could be fit to gait data in general. See for example [<xref ref-type="bibr" rid="B62">62</xref>,<xref ref-type="bibr" rid="B63">63</xref>].</p><table-wrap id="T2" position="float"><label>Table 2</label><caption><p>Statistical comparison of stride periods under different distributional assumptions</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">Child</td><td align="center">No. strides</td><td align="center" colspan="2">Gaussian distribution</td><td align="center" colspan="2">Gamma distribution</td></tr><tr><td /><td /><td colspan="4"><hr /></td></tr><tr><td /><td /><td align="center"><italic>u</italic><sub><italic>Z</italic></sub></td><td align="center"><italic>&#963;</italic><sub><italic>Z</italic></sub></td><td align="center"><italic>a</italic></td><td align="center"><italic>b</italic></td></tr></thead><tbody><tr><td align="center">1</td><td align="center">24</td><td align="center">1.36</td><td align="center">0.158</td><td align="center">79.19</td><td align="center">0.0171</td></tr><tr><td align="center">2</td><td align="center">23</td><td align="center">1.74</td><td align="center">0.734</td><td align="center">7.513</td><td align="center">0.232</td></tr><tr><td /><td /><td align="center" colspan="2"><italic>p </italic>= 0.31</td><td align="center" colspan="2"><italic>p </italic>= 0.036</td></tr></tbody></table></table-wrap><p>In brief, the issue of non-normal distributions of measured gait variables or derived parameters, may lead to inaccurate reports of population means and variability and error-prone statistical testing. In fact, as the last example has shown, different distributional assumptions may lead to different statistical conclusions. Without a priori knowledge about the form of the distribution, one possible solution is to use a general mixture distribution to summarize the gait data. When we have some a priori knowledge about the underlying distribution, we can simply summarize the data using a known non-gaussian distribution, such as the gamma distribution exemplified above for the right-skewed stride period data. In either case, it is generally advisable to routinely check for significant departure from normality using such tests for normality as Pearson's Chi-square [<xref ref-type="bibr" rid="B64">64</xref>] or Lilliefors [<xref ref-type="bibr" rid="B57">57</xref>].</p><p>We remark that mixture models typically have a larger number of parameters than simple unimodal models. As a general rule-of-thumb, one should thus consider that mixture models generally require more data points for their estimation [<xref ref-type="bibr" rid="B59">59</xref>]. In particular, note that in any hypothesis test, the requisite sample size is dependent on the anticipated effect size, the desired level of significance and the specified level of statistical power [<xref ref-type="bibr" rid="B65">65</xref>]. For specific guidelines and methodology relating to sample size determination, the reader is referred to literature on sample size considerations in general hypothesis testing [<xref ref-type="bibr" rid="B66">66</xref>], normality testing [<xref ref-type="bibr" rid="B67">67</xref>], and other distributional testing [<xref ref-type="bibr" rid="B68">68</xref>].</p></sec></sec></sec><sec><title>Single-cycle gait curves</title><p>Kinematic, kinetic and metabolic data are often presented in the form of single-cycle curves, representing a time-varying value over one complete gait cycle. Time is often normalized such that the data vary over percentages of the gait cycle rather than absolute time. Examples include curves for joint angles, moments and powers, ground reaction forces, and potential and kinetic energy. Due to variability from stride-to-stride, these measurements do not generate a single curve, but a family of curves, each one slightly different from the other. We will consider a family of gait curves as realizations of a random function [<xref ref-type="bibr" rid="B69">69</xref>-<xref ref-type="bibr" rid="B71">71</xref>]. Let <italic>X</italic><sub><italic>j </italic></sub>(<italic>t</italic>) denote a discrete time function, i.e. a gait curve, where for convenience and without loss of generality, <italic>t </italic>is a positive integer and <italic>t </italic>= 1,..., 100. We further assume that the differences among curves at each point in time are independently normally distributed. Each sample curve, <italic>X</italic><sub><italic>j </italic></sub>(<italic>t</italic>), can thus be represented as [<xref ref-type="bibr" rid="B70">70</xref>],</p><p><italic>X</italic><sub><italic>j </italic></sub>(<italic>t</italic>) = <italic>f</italic>(<italic>t</italic>) + <italic>&#949;</italic><sub><italic>j </italic></sub>(<italic>t</italic>) <italic>j </italic>= 1,..., <italic>N </italic><italic>t </italic>= 1,..., 100 &#160;&#160;&#160; (8)</p><p>where <italic>f</italic>(<italic>t</italic>) is the true underlying mean function, <italic>&#949;</italic><sub><italic>j </italic></sub>(<italic>t</italic>) ~ <inline-graphic ns0:href="1743-0003-2-22-i13.gif" /> (0, <italic>&#963;</italic><sub><italic>j </italic></sub>(<italic>t</italic>)<sup>2</sup>) are independent, normally distributed, gaussian random variables with variance <italic>&#963;</italic><sub><italic>j </italic></sub>(<italic>t</italic>)<sup>2 </sup>and <italic>N </italic>is the number of curves observed. With this formulation in mind, we now address four prevalent challenges in analyzing gait curves, namely, undesired phase variation, robust estimation of spread, the difficulty with landmark analysis and lastly, the comparison of curves as whole objects rather than as disconnected points.</p><sec><title>Phase variation</title><p>It has been recognized that within a sample of single-cycle gait curves, there is both amplitude and phase variation [<xref ref-type="bibr" rid="B71">71</xref>-<xref ref-type="bibr" rid="B73">73</xref>]. Typically, when we describe variability in gait curves, we refer to amplitude variability. However, unchecked phase variation, that is the temporal misalignment of curves, can often lead to inflated amplitude variability estimates [<xref ref-type="bibr" rid="B72">72</xref>,<xref ref-type="bibr" rid="B73">73</xref>]. Computing cross-sectional averages over a family of malaligned gait curves can lead to the cancellation of critical shape characteristics and landmarks [<xref ref-type="bibr" rid="B74">74</xref>]. This issue presents a significant challenge when summarizing a series of curves for clinical interpretation and treatment planning. On the one hand, the presentation of a large number of different curves can be overwhelmingly difficult to assimilate. On the other hand, a prototypical average curve which does not reflect the features of the individual curves is equally uninformative.</p><p>Curve registration [<xref ref-type="bibr" rid="B71">71</xref>] is loosely the process of temporally aligning a set of curves. More precisely, it is the alignment of curves by minimizing discrepancies from an iteratively estimated sample mean or by allineating specific curve landmarks. Sadeghi et al. demonstrated the use of curve registration, particularly to reduce intersubject variability in angular displacement, moment and power curves [<xref ref-type="bibr" rid="B72">72</xref>,<xref ref-type="bibr" rid="B73">73</xref>]. Additionally, they reported that curve characteristics, namely, first and second derivatives and harmonic content were preserved while peak hip angular displacement and power increased upon registration [<xref ref-type="bibr" rid="B72">72</xref>]. This latter finding confirms that averaging unregistered curves may eliminate useful information.</p><p>Judging by the few gait papers employing curve registration, the method appears largely unknown among the quantitative gait analysis community. Here, we briefly outline the the global registration criterion method [<xref ref-type="bibr" rid="B71">71</xref>,<xref ref-type="bibr" rid="B75">75</xref>].</p><p>Since each gait curve is a discrete set of points, it is useful to estimate a smooth sample function for each observed sample curve. Given the periodic nature of gait curves, the Fourier transform provides an adequate functional representation of each curve. The basic principle is then to repeatedly align a set of sample functions to an iteratively estimated mean function. The agreement between a sample function and the mean function can be measured by a sum-of-squared error criterion. The goal of registration is to find a set of temporal shift functions such that the evaluation of each sample function at the transformed temporal values minimizes the sum-of-squared error criterion. The sample mean is re-estimated at each iteration with the current set of time-warped curves. As an optimization problem, the curve registration procedure is the iterative minimization of the sum-of-squared criterion <italic>J</italic>,</p><p><inline-graphic ns0:href="1743-0003-2-22-i14.gif" /></p><p>where <italic>N </italic>is the number of sample curves, <italic>T </italic>is the time interval of relevance, <italic>w</italic><sub><italic>i</italic></sub>(&#183;) is the time-warping function and <inline-graphic ns0:href="1743-0003-2-22-i15.gif" /> is the iteratively estimated mean based on the current time-warped curves <italic>X</italic><sub><italic>i </italic></sub>(<italic>w</italic><sub><italic>i </italic></sub>(<italic>s</italic>)). For greater methodological details, the reader is referred to [<xref ref-type="bibr" rid="B71">71</xref>,<xref ref-type="bibr" rid="B72">72</xref>,<xref ref-type="bibr" rid="B75">75</xref>]. This global registration criterion method is only one of several possibilities for curve alignment. Related methods which are applicable to gait data include dynamic time warping based on identified curve landmarks [<xref ref-type="bibr" rid="B41">41</xref>] and latency corrected ensemble averaging [<xref ref-type="bibr" rid="B28">28</xref>].</p><p>We exemplify the impact of accounting for undesirable phase variation using ankle angular displacement data from a child with spastic diplegla. The top left graph of Figure <xref ref-type="fig" rid="F6">6</xref> depicts the unregistered curves, exhibiting excessive dorsiflexion throughout the gait cycle and the absence of the initial valley during loading response. Below this graph are the aligned curves. Note particularly the alignment of the large valley at pre-swing and the peak in swing phase.</p><fig id="F6" position="float"><label>Figure 6</label><caption><p>Accounting for phase variation. On the left, we portray unregistered (top graph) and registered (bottom graph) ankle angle curves from a child with spastic diplegia. On the right are the mean (top) and standard deviation (bottom) curves before (dashed line) and after (solid line) curve registration.</p></caption><graphic ns0:href="1743-0003-2-22-6" /></fig><p>The right column of Figure <xref ref-type="fig" rid="F6">6</xref> indicates that the differences in the mean and standard deviation curves before and after registration are non-trivial, with maximum changes of +15% and -51%, respectively. The post-registration mean curve not only exhibits heightened but shifted peaks (3 &#8211; 5% of the gait cycle). This observation suggests that simple cross-sectional averaging without alignment may not only diminish useful curve features but can also inadvertently misrepresent the temporal position of key landmarks. Inaccurate identification of these landmarks, such as the minimum dorsiflexion at the onset of swing phase in this example, could be problematic when attempting to coordinate spatio-temporal and EMG recordings with kinematic curves. The bottom right graph shows a dramatic decrease in variability after registration, particularly in terminal stance. This finding is in line with the tendency towards variability reduction reported by Sadeghi et al. [<xref ref-type="bibr" rid="B72">72</xref>].</p><p>While curve registration is useful for mitigating unwanted phase variation in gait curves, there may be instances where phase variability is itself of interest [<xref ref-type="bibr" rid="B3">3</xref>]. In such instances, curve registration can still be useful in providing information about the relative temporal phase shifts among curves. Because curve registration actually changes the temporal location of data, it should not be applied in studies concerned with temporal stride dynamic characterizations, such as scaling exponents [<xref ref-type="bibr" rid="B21">21</xref>] or Lyapunov exponents [<xref ref-type="bibr" rid="B44">44</xref>]. At present, only a few gait studies have applied curve registration to manage undesired phase variability. However, the evidence in those studies, along with the example above, supports further research and exploratory application of curve registration to fully grasp its merits and limitations in quantitative gait data analyses. For now, curve registration appears to be the most viable solution to the challenge of summarizing a family of temporally misaligned gait curves. In the ensuing sections, we will demonstrate how curve registration can be used advantageously, in conjunction with other methods to address other curve summary and comparison challenges.</p></sec><sec><title>Robustness of spread estimation</title><p>We have already seen that curve registration can mitigate amplitude variability in a family of gait curves. The robust measurement of variability in gait curves is itself a non-trivial challenge. One may need to estimate the variability in a group of curves for the purposes of classifying a new observation as belonging to the same population, or not [<xref ref-type="bibr" rid="B15">15</xref>]. Alternatively, knowledge of the variability among curves can help in the statistical comparison of two populations of curves [<xref ref-type="bibr" rid="B16">16</xref>], say arising from two different subject groups or pre- and post-intervention.</p><p>As in gait variables, the challenge lies in robustly estimating the spread of a sample of gait curves and to avoid fallacious under or overestimation. The intuitive and perhaps most popular way of estimating curve variability is the calculation of the standard deviation across the sample of curves, for each point in the gait cycle. This yields upper, <italic>U</italic><sub><italic>X</italic></sub>, and lower bands, <italic>L</italic><sub><italic>X</italic></sub>, around the sample of curves, i.e.</p><p><italic>U</italic><sub><italic>X </italic></sub>(<italic>t</italic>) = <italic>&#956;</italic><sub><italic>X </italic></sub>(<italic>t</italic>) + <italic>&#963;</italic><sub><italic>X </italic></sub>(<italic>t</italic>) <italic>t </italic>= 1,..., 100</p><p><italic>L</italic><sub><italic>X </italic></sub>(<italic>t</italic>) = <italic>&#956;</italic><sub><italic>X </italic></sub>(<italic>t</italic>) - <italic>&#963;</italic><sub><italic>X </italic></sub>(<italic>t</italic>) &#160;&#160;&#160; (10)</p><p>where <inline-graphic ns0:href="1743-0003-2-22-i16.gif" />, for <italic>t </italic>= 1,..., 100, is the sample mean curve. Lenhoff et al. argued, by way of empirical examples and systematic cross-validation, that standard deviation bands provide inadequate coverage of the sample curves [<xref ref-type="bibr" rid="B15">15</xref>]. They instead supported the use of bootstrap prediction bands [<xref ref-type="bibr" rid="B76">76</xref>] which, in their study, provided close to the targeted 90% coverage of the sample curves. Two subsequent studies [<xref ref-type="bibr" rid="B16">16</xref>,<xref ref-type="bibr" rid="B17">17</xref>] have adopted the 90% bootstrap bands for the classification of new curves and for the comparison between groups of curves. The usefulness of bootstrap prediction bands for clinical identification of pathological deviations in kinematic curves has also been demonstrated [<xref ref-type="bibr" rid="B77">77</xref>]. In this section, we provide further evidence to support the use of bootstrap prediction bands and argue that they are more stable than standard deviation bands.</p><p>The basic idea of the bootstrap method is to create a large number of bootstrap subsets by resampling the curves <italic>X</italic><sub><italic>j</italic></sub>, <italic>j </italic>= 1,..., <italic>N </italic>with replacement. For each subset, the bootstrap mean and standard deviation are calculated. One then checks how many of the sample curves are "covered" by the bootstrap standard deviation bands. A curve is considered covered, if its maximum absolute standardized difference from the bootstrap mean is less than the bootstrap constant <italic>C</italic>. The number of covered curves averaged over all the bootstrap subsets then yields the coverage probability for the given bootstrap constant, <italic>C</italic>. The upper and lower bootstrap prediction bands can then be written as,</p><p><inline-graphic ns0:href="1743-0003-2-22-i17.gif" /></p><p><inline-graphic ns0:href="1743-0003-2-22-i18.gif" /></p><p>The reader is referred to [<xref ref-type="bibr" rid="B15">15</xref>] for details for practical computer implementation of the above procedure.</p><p>To exemplify issues of robust spread estimation, we consider knee angle curves from a child with spastic diplegia. Initially standard deviation and bootstrap bands are computed for the data prior to curve registration. The maximum absolute deviation from the sample mean curve is reported in Table <xref ref-type="table" rid="T3">3</xref>. For both methods, the maximum spread decreases significantly upon registration, suggesting that there is significant inflated variability in the unaligned curve sample. Once the curves are aligned, one suspicious curve, plotted as a thin dashed line in Figure <xref ref-type="fig" rid="F7">7</xref>, becomes evident. The standard deviation bands around the sample with and without this outlying curve are shown on the left side of Figure <xref ref-type="fig" rid="F7">7</xref>. The maximum spread, that is max<sub><italic>t </italic></sub><inline-graphic ns0:href="1743-0003-2-22-i19.gif" /> and <italic>C</italic><inline-graphic ns0:href="1743-0003-2-22-i19.gif" />, for standard deviation and bootstrap bands, respectively, are labeled on each graph. We see that by removing the outlying curve, both the standard deviation and bootstrap bands become narrower. In fact, as seen in Table <xref ref-type="table" rid="T3">3</xref>, the maximum standard deviation decreases by a dramatic 27%. Thus it appears that the variability among a group of curves, as estimated by both standard deviation and bootstrapping, can be minimized by curve registration and further reduced by the subsequent removal of outlying curves.</p><table-wrap id="T3" position="float"><label>Table 3</label><caption><p>Maximum spread estimates: registered and unregistered data</p></caption><table frame="hsides" rules="groups"><thead><tr><td /><td align="center" colspan="2">Bootstrap bands</td><td align="center" colspan="2">Standard deviation bands</td></tr></thead><tbody><tr><td align="center">Data set</td><td align="center">max <italic>C</italic><graphic ns0:href="1743-0003-2-22-i19.gif" /></td><td align="center">Change</td><td align="center">max <graphic ns0:href="1743-0003-2-22-i19.gif" /></td><td align="center">Change</td></tr><tr><td colspan="5"><hr /></td></tr><tr><td align="center">unregistered data</td><td align="center">12.5</td><td align="center">-</td><td align="center">4.7</td><td align="center">-</td></tr><tr><td align="center">registered data</td><td align="center">9.5</td><td align="center">-24%</td><td align="center">3.96</td><td align="center">-16%</td></tr><tr><td align="center">registered data without outlier</td><td align="center">8.0</td><td align="center">-16%</td><td align="center">2.91</td><td align="center">-27%</td></tr></tbody></table></table-wrap><fig id="F7" position="float"><label>Figure 7</label><caption><p>Estimation of spread in a group of registered knee angle curves from a 13-year old child with spastic diplegia. The left column depicts the standard deviation bands with (top graph) and without (bottom graph) an apparent outlying curve (thin dashed line). The 90% bootstrap prediction bands are plotted on the right, again with (top graph) and without (bottom graph) the outlying curve.</p></caption><graphic ns0:href="1743-0003-2-22-7" /></fig><p>To further understand the robustness properties of the two spread estimators, we generate sensitivity curves using the 45 knee angle curves introduced in Figure <xref ref-type="fig" rid="F4">4</xref>. These curves are first registered to minimize unwanted phase variability. In the case of gait curves, the contaminant is not a single point, but an entire curve. For convenience, we choose the following contaminant,</p><p><inline-graphic ns0:href="1743-0003-2-22-i20.gif" /></p><p>where <italic>&#948; </italic>&#8712; &#8477; and <italic>&#948;</italic><sub><italic>min </italic></sub>&#8804; <italic>&#948; </italic>&#8804; <italic>&#948;</italic><sub><italic>max</italic></sub>. In other words, the contaminant is just a shifted version of the sample mean curve, <inline-graphic ns0:href="1743-0003-2-22-i21.gif" />. For simulating the sensitivity curve, we choose <italic>&#948;</italic><sub><italic>min </italic></sub>= -50 and <italic>&#948;</italic><sub><italic>max </italic></sub>= 50, recognizing that in practice, we would never observe deviations of this magnitude. This large range does however, gives us a more complete picture of the sensitivity curves. We proceed to define the sensitivity curves for the standard deviation and bootstrap estimates as follows,</p><p><inline-graphic ns0:href="1743-0003-2-22-i22.gif" /></p><p><inline-graphic ns0:href="1743-0003-2-22-i23.gif" /></p><p>where <inline-graphic ns0:href="1743-0003-2-22-i24.gif" /> is the variance of the uncontaminated sample and</p><p><inline-graphic ns0:href="1743-0003-2-22-i25.gif" /></p><p>is the variance of the contaminated sample. In the above, <inline-graphic ns0:href="1743-0003-2-22-i26.gif" /> is the mean curve of the contaminated sample. The notations <italic>C</italic><sub><italic>X </italic></sub>and <italic>C</italic><sub><italic>X</italic>, <italic>z </italic></sub>represent the bootstrap constants determined using the original and contaminated data, respectively. In other words, these sensitivity curves will reflect the influence of a contaminant curve, <italic>z</italic>(<italic>t</italic>), on the maximum estimated spread across a group of curves, over the gait cycle. Figure <xref ref-type="fig" rid="F8">8</xref> summarizes the results of evaluating (15) over the simulated contaminants defined in (13).</p><fig id="F8" position="float"><label>Figure 8</label><caption><p>Sensitivity curves for the standard deviation bands and 90% bootstrap estimated prediction bands. Here, each point on a sensitivity curve represents the difference between the maxima of the bands estimated with clean and contaminated data.</p></caption><graphic ns0:href="1743-0003-2-22-8" /></fig><p>We note that, as in the univariate case, the standard deviation exhibits quadratic sensitivity with vertex at the zero deviation curve. This parabolic sensitivity curve indicates that the standard deviation bands are not locally robust to contaminant curves. In contrast, the sensitivity curve for the bootstrap bands is not smooth and quartic in nature. The lack of smoothness is due to the random resampling inherent in the bootstrap method, such that with each contaminant curve, slightly different bootstrap samples are used in estimating the 90% prediction bands. Initially, as the contaminant curve deviates from the mean curve, the sensitivity is negative, meaning that the width of the estimated bands are smaller than those for the uncontaminated data. Indeed, the actual value of the bootstrap constant initially decreases, likely to counter the accompanying sharp increase in the standard deviation bands. In other words, as the standard deviation bands widen, a smaller bootstrap constant is required to cover 90% of the sample curves. However, as the contaminant curve deviates farther from the mean, the slope of standard deviation sensitivity increases in magnitude more slowly. With a smaller change in standard deviation band per unit of deviation of the contaminant curve, the bootstrap constant necessarily increases to maintain 90% coverage. This reasoning accounts for the subsequent increase in the tails of the bootstrap sensitivity curve. Finally, we note that overall, the bootstrap sensitivity curve, although apparently unbounded, traverses a much smaller range than the standard deviation curve. This would suggest that with the kinematic data employed in this example, the bootstrap coverage bands enjoy greater stability than their highly sensitive standard deviation cousins.</p><p>In brief, the foregoing discussion further supports the use of bootstrap coverage bands in robustly summarizing the variability within a family of gait curves. Moreover, curve registration and outlier removal can further tighten the location of the prediction bands.</p></sec><sec><title>Problems with simple parameterizations</title><p>It is common to compare specific landmarks or features of gait curves to gauge the impact of an intervention or to determine differences among different subject populations. However, the identification of curve features is inherently problematic. Indeed, the multiplicity of peaks and valleys across two different groups of curves may be inconsistent. As an example, Figure <xref ref-type="fig" rid="F9">9</xref> portrays the vertical ground reaction force of an able-bodied child on the left with the typical loading response peak, mid-stance valley and terminal stance peak [<xref ref-type="bibr" rid="B78">78</xref>]. On the right is the vertical ground reaction force from the intact side of a child with an above-knee amputation, walking with a prosthetic lower limb. Note that there are at least three distinct peaks in the mean curve (thin dotted line). Attempts to compare the able-bodied and amputee force profiles are encumbered by the unclear choice of corresponding peaks. This is a common pitfall of comparing gait curves on the basis of specific landmarks.</p><fig id="F9" position="float"><label>Figure 9</label><caption><p>Inconsistency in multiplicity and location of local extrema. Graphs portray registered vertical ground reaction force curves from an able-bodied child (left) and from a child with above-knee prosthesis (right). The dotted line on the right is the mean curve while dashed line is the wavelet reconstructed mean curve.</p></caption><graphic ns0:href="1743-0003-2-22-9" /></fig><p>The wavelet transform has been touted as a useful method for uncovering intrinsic trends in data [<xref ref-type="bibr" rid="B79">79</xref>,<xref ref-type="bibr" rid="B80">80</xref>]. Hence, it may be possible to extract an underlying low frequency trend from the amputee force curve for the sake of striking a comparison with the able-bodied curve. To this end, we decomposed the mean force curve for the child with amputation using a 4 &#8211; level coiflet wavelet transform [<xref ref-type="bibr" rid="B81">81</xref>]. We reconstructed the force curve using only the approximation coefficients. The resulting trend line is plotted on the right graph of Figure <xref ref-type="fig" rid="F9">9</xref> as the thick dashed line and more closely resembles the expected force profile. Extraction of the extrema yields plausible peak locations at 17% and 44% of the gait cycle and a valley at 30%. These locations are comparable to those for the able-bodied child (peaks at 12% and 44% and valley at 26% of the gait cycle), but suggest a slightly extended loading response phase.</p><p>The extraction of the trend line in this example illustrates that in some curves, the desired landmarks may be concealed by the fluctuations of higher frequency signal components and hence may be salvageable. However, even when landmarks are clearly identifiable among curves, they reflect only a very microscopic view of the entire curve. For example, two curves could have identical landmarks, but pronounced differences in shape characteristics. We therefore do not advocate the isolated use of simple parameterizations or landmarks for routine comparison of curves. Rather, the comparison of two sets of curves should be based on the entire curve and not isolated parameterizations. We suggest however, that landmark analysis and simple parameterizations can be meaningful as a post-hoc procedure, i.e. investigating how curves are similar or different, only <italic>after </italic>statistically significant differences among curves or lack thereof have been established. We therefore suggest to first statistically compare entire gait curves as unified objects, and reserve landmarks for post-hoc analysis. In the following section, we describe how such a statistical test may be carried out.</p></sec><sec><title>Comparison of gait curves as coherent entities</title><p>If gait curves were strictly deterministic, one could simply define a distance measure between two curves and be done. However, due to stride-to-stride variability, an extension of the univariate statistical test is needed, to determine if one set of curves could have arisen from the same statistical distribution as another. Alternatively, one could test whether the average difference between two sets of curves is approximately zero, within the critical values of an expected distribution of differences. The fundamental challenge is to compare families of gait curves as coherent entities rather than as unconnected, independent points. One way to consider curves as a whole rather than as disjoint points is to give them an appropriate functional representation. One can then compare the functional representations of the curves. Exploiting this principle, Fan and Lin [<xref ref-type="bibr" rid="B70">70</xref>] proposed a general method for comparing two sets of discrete time-sampled curves. In their method, the discrete Fourier transform of the standardized difference between the mean curves of two families of curves is computed. Only selected low frequency components of the transform, which encompass the majority of signal energy, are retained. These coefficients are then subjected to the adaptive Neyman test which yields the probability that the two families of curves have similar means. To the best of our knowledge, the adaptive Neyman statistic [<xref ref-type="bibr" rid="B69">69</xref>] has not yet been applied in the gait literature for the comparison of empirical gait curves. We therefore outline below, in some detail, the proposed procedure that we have adapted from Fan and Lin [<xref ref-type="bibr" rid="B70">70</xref>]. Suppose that we would like to compare two families of gait curves, {<italic>X</italic><sub><italic>j </italic></sub>(<italic>t</italic>), <italic>j </italic>= 1,..., <italic>N</italic><sub><italic>X</italic></sub>} and {<italic>Y</italic><sub><italic>j </italic></sub>(<italic>t</italic>), <italic>j </italic>= 1,..., <italic>N</italic><sub><italic>Y</italic></sub>}, with <italic>t </italic>= 1,..., 100. The null hypothesis is that the difference between the means of the two families of curves is zero. In the random function formulation given by Equation (8), we can write, <italic>H</italic><sub>0 </sub>: <italic>f</italic><sub><italic>X </italic></sub>(<italic>t</italic>) - <italic>f</italic><sub><italic>Y </italic></sub>(<italic>t</italic>) = 0, where <italic>f</italic><sub><italic>X </italic></sub>(<italic>t</italic>) and <italic>f</italic><sub><italic>Y </italic></sub>(<italic>t</italic>) are the true underlying mean curves. For notational convenience, we will let <italic>t </italic>= 0,..., <italic>T </italic>- 1, where we have chosen <italic>T </italic>= 100 in the previous examples. The main steps of the test are as follows.</p><p>1. Compute the sample mean curves, <italic>&#956;</italic><sub><italic>X </italic></sub>(<italic>t</italic>) and <italic>&#956;</italic><sub><italic>Y </italic></sub>(<italic>t</italic>), where <inline-graphic ns0:href="1743-0003-2-22-i27.gif" /> and likewise for <italic>&#956;</italic><sub><italic>Y </italic></sub>(<italic>t</italic>)</p><p>2. Compute the sample variance curves, <inline-graphic ns0:href="1743-0003-2-22-i28.gif" /> and <inline-graphic ns0:href="1743-0003-2-22-i29.gif" />, where <inline-graphic ns0:href="1743-0003-2-22-i30.gif" /> and likewise for <inline-graphic ns0:href="1743-0003-2-22-i29.gif" />.</p><p>3. Align the two mean curves using the global registration criterion method. Denote the registered curves as <inline-graphic ns0:href="1743-0003-2-22-i31.gif" /> and <inline-graphic ns0:href="1743-0003-2-22-i32.gif" />. This step does not appear in the original formulation of Fan and Lin [<xref ref-type="bibr" rid="B70">70</xref>].</p><p>4. Compute the standardized difference <italic>Z</italic>(<italic>t</italic>) between the registered means,</p><p><inline-graphic ns0:href="1743-0003-2-22-i33.gif" /></p><p>5. Compute the discrete Fourier decomposition, <inline-graphic ns0:href="1743-0003-2-22-i34.gif" />, of the standardized difference,</p><p><inline-graphic ns0:href="1743-0003-2-22-i35.gif" /></p><p><inline-graphic ns0:href="1743-0003-2-22-i36.gif" /></p><p>where <italic>k </italic>= 0,..., <italic>T</italic>/2, Real(&#183;) and Imag(&#183;) denote the real and imaginary components of the complex Fourier coefficient <inline-graphic ns0:href="1743-0003-2-22-i34.gif" />, respectively, and <italic>k </italic>denotes the Fourier frequency.</p><p>6. Form a new vector of coefficients <bold>E</bold>, of length <italic>T </italic>+ 1, by pairing real and imaginary coefficients of the complex Fourier coefficients, <inline-graphic ns0:href="1743-0003-2-22-i34.gif" />, as follows,</p><p><inline-graphic ns0:href="1743-0003-2-22-i37.gif" /></p><p>7. Estimate the adaptive Neyman statistic, <italic>T</italic><sub><italic>AN</italic></sub>(<bold>E</bold>) for the vector defined above. This proceeds in two steps.</p><p>(a) Determine the optimal the number of coefficients to retain to maximize <inline-graphic ns0:href="1743-0003-2-22-i38.gif" />, where <italic>E</italic><sub><italic>i </italic></sub>are the elements of the vector defined above and 1 &lt;<italic>m </italic>&lt;<italic>T </italic>+ 1. This optimal value of <italic>m</italic>, denoted <inline-graphic ns0:href="1743-0003-2-22-i39.gif" />, maximizes the power of the adaptive Neyman statistic [<xref ref-type="bibr" rid="B70">70</xref>]. The maximum statistic value is written as,</p><p><inline-graphic ns0:href="1743-0003-2-22-i40.gif" /></p><p>where <italic>Var</italic>(<italic>E</italic><sup>2</sup>), is the variance of the square of the elements of <italic>E </italic>obtained in step 6.</p><p>(b) Let <italic>K </italic>= ln(<italic>T </italic>ln <italic>T</italic>). Compute the following final transformed test statistic value [<xref ref-type="bibr" rid="B70">70</xref>],</p><p><inline-graphic ns0:href="1743-0003-2-22-i41.gif" /></p><p>Here, we have explicited indicated that the statistic has been computed for the vector <bold>E </bold>of Fourier coefficients. Asymptotically, this statistic has an exponential of an exponential distribution [<xref ref-type="bibr" rid="B69">69</xref>], that is, <italic>P</italic>(<bold>T</bold><sub><italic>AN </italic></sub>&#8804; <italic>x</italic>) &#8594; exp(-exp(-<italic>x</italic>)), as <italic>T </italic>becomes arbitrarily large.</p><p>8. Estimate the p-value of the computed test statistic value, <italic>T</italic><sub><italic>AN</italic></sub>(<bold>E</bold>), by Monte Carlo simulation of a large number, say 10<sup>6</sup>, of vectors, <bold>Y</bold><sub><italic>i</italic></sub>, <italic>i </italic>= 1,..., 10<sup>6</sup>, each of length <italic>T </italic>and whose elements are drawn from a standard normal distribution, i.e. <bold>Y</bold><sub><italic>i </italic></sub>~ <inline-graphic ns0:href="1743-0003-2-22-i13.gif" />(0, 1), &#8704;<italic>i</italic>. The rationale is that when two sets of curves arise from the same random function, the standardized differences of their Fourier coefficients are normally distributed around 0. For each normal vector, <bold>Y</bold><sub><italic>i</italic></sub>, evaluate <italic>T</italic><sub><italic>AN</italic></sub>(<bold>Y</bold><sub><italic>i</italic></sub>) as in step 7 above. When the null hypothesis of no differences is true, the probability of observing an adpative neyman statistic as extreme as <italic>T</italic><sub><italic>AN</italic></sub>(<bold>E</bold>) is estimated as,</p><p><inline-graphic ns0:href="1743-0003-2-22-i42.gif" /></p><p>where <italic>H</italic>(&#183;) is the heaviside function, where <italic>H</italic>(<italic>x</italic>) = 1 only if <italic>x </italic>&gt; 0 and is 0 otherwise. In the examples below, we simulated 10<sup>6 </sup>such vectors to estimate the probability of observing <italic>T</italic><sub><italic>AN</italic></sub>.</p><p>We exemplify the above procedure with two kinematic data sets taken from a child with an above-knee amputation, wearing two different types of prosthetic devices. Of interest is whether the use of a swing phase control mechanism within the prosthetic knee affects gait. Figure <xref ref-type="fig" rid="F10">10</xref> depicts the ankle angular displacements with (dashed line) and without (solid line) the swing phase control mechanisms. By visual inspection, the curves look similar in magnitude and exhibit a slight phase difference. We would expect statistical testing to conclude that the curves are not different. The top right graph is the standardized difference between the registered mean curves. Note that most values fluctuate around 0. The bottom right graph is a stem plot of selected Fourier coefficients of the Fourier transform of the standardized difference. Note the concentration of energy in the low frequencies and the distribution of coefficients above and below 0. It is thus not surprising that the result of the adaptive Neyman test statistic, yielded <italic>T</italic><sub><italic>AN </italic></sub>= -9.01 which corresponds to <italic>p </italic>= 1 for <inline-graphic ns0:href="1743-0003-2-22-i39.gif" /> = 2. In other words, it is very likely that the two groups of curves came from the same distribution of random functions. In the present context, the swing phase mechanism had no effect on the ankle angular displacement.</p><fig id="F10" position="float"><label>Figure 10</label><caption><p>Comparison of ankle angle curves (left pane) from an above-knee amputee using a prosthetic knee without (solid line) and with (dashed line) a swing phase control mechanism. The right pane depicts the standardized difference between the registered mean curves for the two groups (top graph) and the corresponding first 30 coefficients of its discrete Fourier transform.</p></caption><graphic ns0:href="1743-0003-2-22-10" /></fig><p>To illustrate the statistical detection of differences, we draw upon a second example involving kinematic curves from an adult subject pre- and post-surgical replacement of the ankle. In Figure <xref ref-type="fig" rid="F11">11</xref>, the pre-surgery curves do not have well resolved peaks or valleys whereas post-surgically, distinct peaks and valleys emerge with substantial magnitude. On the basis of this visual inspection, one would anticipate that statistical testing should indicate that the pre- and post-surgery curves are indeed different. The standardized difference between the registered mean curves exhibits relatively large fluctuations around 0 and the retained Fourier coefficients are nearly all positive, resulting in a positively skewed coefficient distribution. The adaptive Neyman statistic value for these coefficients is <italic>T</italic><sub><italic>AN </italic></sub>= 5.99 corresponding to <italic>p </italic>= 2.6 &#215; 10<sup>-5 </sup>with <inline-graphic ns0:href="1743-0003-2-22-i39.gif" /> = 6. Hence, the statistical test indicates that there is strong evidence for rejecting the null hypothesis. It appears that surgery has significantly altered the gait curves. Once significant statistical difference has been established, one can then seek to identify specific characteristics which differentiate the two sets of curves. For example, the post-surgical curves exhibit a well-defined valley, towards plantar flexion at toe-off and a strong first dorsiflexion peak in terminal stance. Both of these extrema are absent in the presurgery curves.</p><fig id="F11" position="float"><label>Figure 11</label><caption><p>Comparison of ankle angle curves (left pane) from one individual before and after total ankle replacement surgery. The right pane portrays the standardized difference between the registered mean curves of each group (top graph) and the first 30 coefficients of its discrete Fourier transform.</p></caption><graphic ns0:href="1743-0003-2-22-11" /></fig><p>Note that we have not said anything about the requisite sample sizes for the statistical comparison of gait curves. Clearly, as in unidimensional power analysis [<xref ref-type="bibr" rid="B65">65</xref>], the required sample size depends on the effect size, significance level and specified power. To the best of our knowledge, no power-sample size tables have been derived for the adaptive Neyman statistic at the time of writing. For insights on the topic, the interested reader can refer to authoritative works [<xref ref-type="bibr" rid="B65">65</xref>,<xref ref-type="bibr" rid="B82">82</xref>] on power analysis in the univariate case. The statistical testing demonstrated here can be extended to compare more than two groups of curves, using high-dimensional analysis of variance [<xref ref-type="bibr" rid="B70">70</xref>]. Further, when the standardized difference curve is not smooth, wavelet denoising can be used to identify the frequency bands where the majority of signal energy is concentrated [<xref ref-type="bibr" rid="B69">69</xref>]. The adaptive Neyman statistic introduced here is only one of several possibilities for objectively and rigorously testing differences among curves. Other alternatives include an ANOVA test for functional data [<xref ref-type="bibr" rid="B83">83</xref>] and functional canonical correlation analysis [<xref ref-type="bibr" rid="B84">84</xref>]. The procedure outlined in this section formalizes the comparison of gait curves as coherent entities. The method provides a means of statistically confirming overall similarities and differences that we may detect by visual inspection, but may have difficulty quantifying with conventional time and frequency domain parameterizations.</p></sec></sec><sec><title>Recommendations</title><p>We summarize the foregoing discussions by proposing some heuristic guidelines for dealing with the aforementioned variability issues in gait variables and curves. For gait variables or parameters, the suggested solution pathways are shown in Figure <xref ref-type="fig" rid="F12">12</xref>.</p><fig id="F12" position="float"><label>Figure 12</label><caption><p>Heuristic guidelines for summary of gait variables or parameters.</p></caption><graphic ns0:href="1743-0003-2-22-12" /></fig><p>For gait curves, the suggested procedures for summary and comparison are summarized in Figure <xref ref-type="fig" rid="F13">13</xref>. A few comments beyond the above discussions are in order. Note that robust estimation is suggested in the summary of gait curves, as after registration, there may still be curves which appear atypical, in amplitude or overall shape. Location estimation of gait curves was only discussed in the context of the adaptive Neyman test, but is included in Figure <xref ref-type="fig" rid="F13">13</xref> for completeness. In the comparison of curves, post-hoc analysis would encompass the comparisons of conventional curve parameterizations or landmarks (e.g. peaks and valleys), as investigative procedures to explain the formally established statistical differences or lack thereof.</p><fig id="F13" position="float"><label>Figure 13</label><caption><p>Heuristic guidelines for the summary (on the left) and comparison (on the right) of gait curves.</p></caption><graphic ns0:href="1743-0003-2-22-13" /></fig></sec><sec><title>Future directions</title><p>This paper has only skimmed the tip of the iceberg in the discussion and demonstration of several promising analytical approaches for practically addressing variability issues in gait data summary and comparison. The topics of curve registration and bootstrap estimates of curve variability, although not necessarily new to gait data analyses, have been seldom studied and applied in the gait research community. The handful of studies to date on these subjects, have provided strong initial evidence for potentially improving the rigor and objectivity of gait data interpretation. Examples in the present paper lend further credence to these methods. Systematic comparisons of these techniques with conventional parameterizations, summary statistics, and even expert interpretation of gait data, would lead to a greater appreciation of their relative merits and limitations in gait data analyses. For example, would the use of registration and bootstrapping to consolidate gait data improve the consistency of clinical decision-making? Given the propensity for variability inflation in gait data, the topic of robust estimation needs to be studied in greater depth, in terms of contaminant influences and possibly adaptive estimators [<xref ref-type="bibr" rid="B49">49</xref>]. Likewise, the rigorous statistical comparison of gait curves as coherent entities rather than uncorrelated sets of points, is a promising area of research in gait variability analyses. This stream of study is only in the embryonic stages but promises to strengthen the comparison of quantitative gait data and to complement its subjective interpretation, a pratice which has been debated in literature [<xref ref-type="bibr" rid="B85">85</xref>-<xref ref-type="bibr" rid="B87">87</xref>].</p></sec><sec><title>Authors' contributions</title><p>T. Chau wrote the entire manuscript and carried out the majority variability analyses reported herein.</p><p>S. Redekop collected most of the empirical data reported herein, carried out all the kinematic and kinetic data analyses, wrote programs for data extraction, identified the datasets and helped in their interpretation.</p><p>S. Young provided the literature review for the manuscript and contributed significantly to revising various drafts.</p></sec></body></data>